{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v40JvQ33K3BX"
   },
   "source": [
    "# Машинное обучение, DS-поток\n",
    "## Задание 1.12\n",
    "\n",
    "\n",
    "**Правила:**\n",
    "\n",
    "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ds21_bot`.\n",
    "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Прислать нужно ноутбук в формате `ipynb`.\n",
    "* Решения, размещенные на каких-либо интернет-ресурсах не принимаются. Публикация решения может быть приравнена к предоставлении возможности списать.\n",
    "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него.\n",
    "* Никакой код из данного задания при проверке запускаться не будет.\n",
    "* За задание можно получить до **20 баллов**:\n",
    "  * часть 1 &mdash; 5 баллов, \n",
    "  * часть 2 &mdash; 15 баллов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2_WRDz13hmA"
   },
   "outputs": [],
   "source": [
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Obf0YFzsi9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torchvision.datasets import coco\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcZDet3OvPvc"
   },
   "source": [
    "# Image Captioning\n",
    "\n",
    "### План работы.\n",
    "**Часть 1.** В качестве энкодера, переводящего изображения в векторы, взять предобученную сеть inception v3. Эту часть можно пропустить с потерей баллов, взяв готовые векторы, ссылка на которые есть во второй части.\n",
    "\n",
    "**Часть 2.** В качестве декодера, переводящего векторы в текст описания, взять LSTM; обучить декодер на векторизованных изображениях датасета [MSCOCO](http://cocodataset.org/#download).\n",
    "\n",
    "Задача image captioning заключается в генерации текстовых описаний к изображениям. В большинстве случаев используются архитектуры из двух частей &mdash; энкодера, переводящего изображения в векторы, и декодера, генерирующего текст по этим векторам. Поскольку обучать энкодер и декодер вместе &mdash; вычислительно затратная операция, мы решили разделить их и разбить домашнее задание на две части."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt0p0yhzTppI"
   },
   "source": [
    "## Часть 1. Векторизация изображений\n",
    "\n",
    "![](https://docs.google.com/uc?export=download&id=18Dp1dng87NbnhlBHnFVoEaiKlNZEXybh)\n",
    "\n",
    "\n",
    "В этой части вам предстоит выполнить первый этап задачи image captioning &mdash; обучения энкодера для перевода изображений в векторы. \n",
    "\n",
    "**Замечание.** Выполнение этого задания требует больших вычислительных ресурсов. Рекомендуется использовать Google Colab или ноутбук с GPU. Кроме того, вы можете пропустить эту часть с потерей баллов и начать сразу с генерации текстов по векторам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiCuc2biPk64"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Для решения задачи используется датасет COCO (Common Objects in Context), содержащий большое количество изображений для image captioning.\n",
    "\n",
    "Загрузим данные. Понадобится около 18 гигабайт на диске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDztoHNDTppK"
   },
   "outputs": [],
   "source": [
    "# Загрузим данные с http://cocodataset.org/#download\n",
    "!curl http://images.cocodataset.org/zips/train2017.zip > train2017.zip\n",
    "!curl http://images.cocodataset.org/zips/val2017.zip > val2017.zip\n",
    "!curl http://images.cocodataset.org/annotations/annotations_trainval2017.zip > annotations_trainval2017.zip\n",
    "\n",
    "#!unzip annotations_trainval2017.zip\n",
    "#!unzip train2017.zip > log\n",
    "#!unzip val2017.zip > log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHvUTa6dhIpd"
   },
   "outputs": [],
   "source": [
    "!unzip train2017.zip > log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3s7E-9qXX2a"
   },
   "source": [
    "Чтобы упростить вам задачу, мы не будем обучать энкодер, а возьмём предобученные веса для сети. Предлагается  использовать сеть `ResNet`, но вы можете выбрать любую сеть для изображений на свой вкус.\n",
    "\n",
    "Важно помнить, что перед передачей в сеть изображения нужно предобработать. При этом, у каждой сети свои требования к размеру входному изображению и способу нормирования каналов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JobSnxxyTpqA"
   },
   "outputs": [],
   "source": [
    "# предобработка для Inception-v3\n",
    "preprocess = transforms.Compose((\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANBqZ5Jl3-qT"
   },
   "source": [
    "Инициализируем `DataLoader` для генерации батчей. Значение `batch_size` выберите в зависимости от имеющихся вычислительных ресурсов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgwKRdXC39cG"
   },
   "outputs": [],
   "source": [
    "coco_train = coco.CocoCaptions(\n",
    "    \"./train2017/\", \"./annotations/captions_train2017.json\", transform=preprocess\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=coco_train, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOsCRXkgnpg"
   },
   "source": [
    "Модифицируем предобученную сеть для генерации векторов по изображениям. Как было сказано выше, вы можете выбрать любую архитектуру, но размер выхода должен быть равным 2048. \n",
    "\n",
    "В `torchvision.models` содержатся сети, предобученные на задаче классификации. Поэтому для их использования надо удалить из них последний слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdfrAnmhTpqF"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # загрузим предобученную сеть\n",
    "        net = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in net.parameters():\n",
    "            # отключим подсчёт градиента для параметра\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # выберем все слои, кроме последнего. Воспользуйтесь методом children() \n",
    "        # у класса предобученной сети\n",
    "        modules = <...>\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Xl8vQKTpqM"
   },
   "outputs": [],
   "source": [
    "model = EncoderCNN()\n",
    "# установим режим evaluation. Это отключит все dropout-ы в сети\n",
    "model = nn.DataParallel(model.train(False).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOi72A7qTpqY"
   },
   "source": [
    "### Векторизация изображений\n",
    "\n",
    "Получим векторы представления изображений с помощью модели.\n",
    "\n",
    "**Замечание.** Требует большого объёма RAM. Если памяти не хватает, то следует каждые $K$ итераций сохранять векторы на диск и очищать память.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwaWr7EUTpqZ"
   },
   "outputs": [],
   "source": [
    "vectors, captions = [], []\n",
    "\n",
    "for img_batch, capt_batch in tqdm(data_loader):\n",
    "    capt_batch = list(zip(*capt_batch))\n",
    "    img_batch = Variable(img_batch, volatile=True)\n",
    "    # получите векторы изображений из модели\n",
    "    vec_batch = <...>\n",
    "    \n",
    "    captions.extend(capt_batch)\n",
    "    vectors.extend([vec for vec in vec_batch])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZCVtYx1b-zK"
   },
   "source": [
    "Переведите тексты описаний в нижний регистр и токенизируйте их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3IgM1rbTpqf"
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "# токенизируем описания текстов\n",
    "captions_tokenized = <...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JyLKhmQTpqj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Исходный текст:\\n%s\\n\\n\" % '\\n'.join(captions[0]))\n",
    "print(\"Токенизированный текст:\\n%s\\n\\n\"% '\\n'.join(captions_tokenized[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uhhkWFdTpqo"
   },
   "source": [
    "### Сохранение векторов\n",
    "\n",
    "Если вы делаете это задание на `Colab`, сохраните папку с данными себе на диск или скачайте её, чтобы можно было переиспользовать в следующей части задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM1heKhuTpqp"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "np.save(\"./data/image_codes.npy\", np.asarray(vectors))\n",
    "\n",
    "with open('./data/captions_tokenized.json', 'w') as f_cap:\n",
    "    json.dump(captions_tokenized, f_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHxrEKza-mTQ"
   },
   "source": [
    "## Часть 2. Генерации описания изображения.\n",
    "\n",
    "![img](https://docs.google.com/uc?export=download&id=1S084CyPJx-Y8qO14rq04m3QPH3auWdni)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NqcwsXc4NJF"
   },
   "source": [
    "### Загрузка и чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aqITPOVMFXM"
   },
   "source": [
    "Загрузим векторы изображений, полученные в первой части задания. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWQ4f_QE12Hk"
   },
   "source": [
    "\n",
    "Если вы не выполнили ту часть задания и не претендуете на баллы за неё, <a href=\"https://drive.google.com/file/d/16Lg6FjUfzGpvHgzHRHS9VD2L3Rv_862P/view?usp=sharing\">загрузите</a> изображения, которые мы векторизовали за вас при помощи `ResNet`. \n",
    "Поскольку этот файл большой, для его скачивания с Google.Drive применим технику описанную в <a href=\"https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99\">этой статье</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIpsBzOk-mTS"
   },
   "outputs": [],
   "source": [
    "# скачаем векторы изображений через wget\n",
    "!wget --load-cookies /tmp/cookies.txt\\\n",
    "  \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=16Lg6FjUfzGpvHgzHRHS9VD2L3Rv_862P' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=16Lg6FjUfzGpvHgzHRHS9VD2L3Rv_862P\" \\\n",
    "  -O handout.tar.gz\\\n",
    "  && rm -rf /tmp/cookies.txt\n",
    "\n",
    "# распакуем архив, содержащий папку data со всеми данными\n",
    "!tar -xzf handout.tar.gz\n",
    "\n",
    "# проверим содержимое\n",
    "!ls -lha data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEhWLr6y4T8b"
   },
   "source": [
    "Прочтём данные из файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pOKKDNO-mTl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "img_codes = np.load(\"data/image_codes.npy\")\n",
    "captions = json.load(open('data/captions_tokenized.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_llTvOs-mTh"
   },
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TwBGMj6-mT1"
   },
   "source": [
    "Убедимся, что векторы для всех изображений имеют размер 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cskqB7XJ-mT3"
   },
   "outputs": [],
   "source": [
    "print(\"Размерность всех данных\", img_codes.shape)\n",
    "print(img_codes[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xe_imqI8p1L"
   },
   "source": [
    "Каждое изображение имеет 5 вариантов текстового описания, причём все тексты токенизированы и приведены в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAX_-wGi8quL"
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(captions[228]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUbjzwB0-mUC"
   },
   "source": [
    "Разобъём текст на токены. Заведём отдельные токены для начала и конца текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xh_-4sg-mUE"
   },
   "outputs": [],
   "source": [
    "START_TOKEN, END_TOKEN = '#BOS', '#EOS'\n",
    "captions = [[[START_TOKEN] + caption.split(' ') + [END_TOKEN]\\\n",
    "            for caption in img_caption_list]\\\n",
    "            for img_caption_list in captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIkxgI5m-rjR"
   },
   "source": [
    "Вычислим количество уникальных слов, встречающихся в описании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX_XGl-2iG0-"
   },
   "outputs": [],
   "source": [
    "word_set = <...>\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPGD0WoG-mUM"
   },
   "source": [
    "Заметим, что слов очень много. Для того, чтобы ограничить словарь, выберем наиболее частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKyekY8u-mUO"
   },
   "outputs": [],
   "source": [
    "word_counts = Counter()\n",
    "\n",
    "# Посчитайте количество вхождений каждого уникального слова\n",
    "<...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6mKs84s-2kZ"
   },
   "source": [
    "Сформируем словарь из наиболее частых токенов. Нам понадобятся 4 вспомогательных токена: \n",
    "* `#UNK` &mdash; слово, не встречающееся в словаре;\n",
    "* `#BOS` &mdash; начало текста;\n",
    "* `#EOS` &mdash; конец текста;\n",
    "* `#PAD` &mdash; пустой токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHrc9xFu-mUX"
   },
   "outputs": [],
   "source": [
    "vocab  = ['#UNK', '#BOS', '#EOS', '#PAD']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n",
    "n_tokens = len(vocab)\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8tGIjh0-mUd"
   },
   "outputs": [],
   "source": [
    "eos_ix = word_to_index['#EOS']\n",
    "unk_ix = word_to_index['#UNK']\n",
    "pad_ix = word_to_index['#PAD']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Конвертирование списка строк в целочисленный тензор \"\"\"\n",
    "\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-e57eUCiubJ"
   },
   "source": [
    "Проверим работу функции на случайном описании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TZg9f27-mUj"
   },
   "outputs": [],
   "source": [
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tny4seOe-mUq"
   },
   "source": [
    "### Архитектура декодера\n",
    "\n",
    "В качестве модели будем использовать `CaptionNet`.\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)\n",
    "\n",
    "\n",
    "Эта архитектура &mdash; своеобразная модификация рекуррентной языковой модели. Основная особенность &mdash; начальные скрытые  состояния LSTM вычисляются по изображению при помощи свёрточной нейронной сети. Мы уже получили векторы изображений в первой части задания. Поэтому сейчас нам не придётся использовать свёрточные слои в явном виде.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nIIMawF-mUx"
   },
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256,\n",
    "                 cnn_feature_size=2048):\n",
    "        \"\"\" \n",
    "        Инициализация CaptionNet\n",
    "        \n",
    "        Параметры.\n",
    "        1) n_tokens - размер словаря для текстовых описаний,\n",
    "        2) emb_size - размер эмбеддингов для токенов из словаря,\n",
    "        3) lstm_units - размер скрытого состояния LSTM,\n",
    "        4) cnn_features_size - размер эмбеддинга изображений.\n",
    "        \"\"\"\n",
    "\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        # слой, переводящий эмбеддинг картинки в вектор, \n",
    "        # который будет использован как начальное состояние h в LSTM\n",
    "        self.cnn_to_h0 = <...>\n",
    "        # слой, переводящий эмбеддинг картинки в вектор, \n",
    "        # который будет использован как начальное состояние c в LSTM\n",
    "        self.cnn_to_c0 = <...>\n",
    "        # эмбеддинги для токенов описаний\n",
    "        self.emb = <...>\n",
    "        # рекуррентный слой\n",
    "        self.lstm = <...>\n",
    "        # слой для вычисления логитов\n",
    "        self.logits = <...>\n",
    "        \n",
    "    def forward(self, image_vectors, captions_ix):\n",
    "        \"\"\" Применение сети \"\"\"\n",
    "\n",
    "        initial_cell = self.cnn_to_c0(image_vectors)\n",
    "        initial_hid = self.cnn_to_h0(image_vectors)\n",
    "        \n",
    "        # получим эмбеддинги описаний\n",
    "        captions_emb = <...>\n",
    "        # применяем lstm, в качестве начальных состояний берем initial_cell и initial_hid\n",
    "        lstm_out, _ = <...>\n",
    "        # вычислим логиты\n",
    "        logits = <...>\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5XrSrOxm3kN"
   },
   "source": [
    "Инициализируем сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq-MgNXJ-mU3"
   },
   "outputs": [],
   "source": [
    "network = CaptionNet(n_tokens)\n",
    "checkpoint_path = Path('best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzxzA24vmTSk"
   },
   "source": [
    "Сгенерируем случайные векторы для простого тестирования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4nz0kkJ-mU8"
   },
   "outputs": [],
   "source": [
    "dummy_img_vec = torch.randn(len(captions[0]), 2048)\n",
    "dummy_capt_ix = torch.tensor(as_matrix(captions[0]), dtype=torch.int64)\n",
    "\n",
    "dummy_logits = network.forward(dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "print('shape:', dummy_logits.shape)\n",
    "assert dummy_logits.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avz-9g3smMPA"
   },
   "source": [
    "Реализуем вычисление функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbQFLu3N-mVD"
   },
   "outputs": [],
   "source": [
    "def compute_loss(network, image_vectors, captions_ix):\n",
    "    \"\"\" Подсчёт функции потерь \"\"\"\n",
    "    \n",
    "    captions_ix_inp = captions_ix[:, :-1].contiguous()\n",
    "    captions_ix_next = captions_ix[:, 1:].contiguous()\n",
    "    \n",
    "    # получим логиты, применив сеть\n",
    "    logits_for_next = network.forward(image_vectors, captions_ix_inp)    \n",
    "    loss = F.cross_entropy(\n",
    "        logits_for_next.view(-1, n_tokens), \n",
    "        captions_ix_next.view(-1), \n",
    "        ignore_index=pad_ix\n",
    "    )\n",
    "    \n",
    "    return torch.stack([loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mF7RnLCBmF2p"
   },
   "source": [
    "Посчитаем функцию потерь на простом примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUnLSu-D-mVH"
   },
   "outputs": [],
   "source": [
    "dummy_loss = compute_loss(network, dummy_img_vec, dummy_capt_ix)\n",
    "dummy_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESD4zMYV-mVK"
   },
   "source": [
    "Инициализируем оптимизатор для нейронной сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfdNADYC-mVK"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WhhQp0W-mVO"
   },
   "source": [
    "### Обучение сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3VhO8dl9VPJ"
   },
   "source": [
    "Разделим данные на обучение и валидацию. Отведём на валидацию 20% данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBZT37Cc-mVP"
   },
   "outputs": [],
   "source": [
    "captions = np.array(captions)\n",
    "train_img_codes, val_img_codes, train_captions, val_captions = train_test_split(\n",
    "    img_codes, captions, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjDTcnDw9soH"
   },
   "source": [
    "Реализуем генератор батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRcqNRTs-mVT"
   },
   "outputs": [],
   "source": [
    "def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n",
    "    '''\n",
    "    Генератор батчей.\n",
    "    Параметры.\n",
    "    1) img_codes - векторы изображений,\n",
    "    2) captions - описания изображений,\n",
    "    3) batch_size - размер батча,\n",
    "    4) max_caption_len - ограничение сверху на длину описания.\n",
    "    '''\n",
    "\n",
    "    # выбираем случайные изображения\n",
    "    random_image_ix = np.random.randint(0, len(img_codes), size=batch_size)\n",
    "    batch_images = img_codes[random_image_ix]\n",
    "    \n",
    "    # берём все описания для выбранных случайных изображений\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    # берём для каждого изображения ровно одно описание\n",
    "    batch_captions = list(map(choice, captions_for_batch_images))\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return torch.tensor(batch_images, dtype=torch.float32),\\\n",
    "           torch.tensor(batch_captions_ix, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i97w7IXnAtT"
   },
   "source": [
    "Сгенерируем один случайный батч."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxzIkuFn-mVW"
   },
   "outputs": [],
   "source": [
    "generate_batch(img_codes, captions, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FFVcqCp-mVa"
   },
   "source": [
    "### Цикл обучения\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGUlHPp7_REv"
   },
   "source": [
    "Подберите размер батча и количество эпох в зависимости от имеющихся вычислительных ресурсов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4WvZhTI-mVa"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "n_batches_per_epoch = len(train_img_codes) // batch_size \n",
    "n_validation_batches = len(train_img_codes) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BCAmUfmnnnG"
   },
   "source": [
    "Обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTAkTh0Y-mVe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_val_loss = 1000.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    network.train(True)\n",
    "    \n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        # считаем значение функции потерь\n",
    "        loss_t = compute_loss(\n",
    "            network, \n",
    "            *generate_batch(train_img_codes, train_captions, batch_size)\n",
    "        )\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # делаем обратное распространение ошибки\n",
    "        loss_t.backward()\n",
    "        # делаем шаг оптимизатора\n",
    "        optim.step()\n",
    "        train_loss += loss_t.item()\n",
    "\n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss = 0\n",
    "    network.train(False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        loss_t = compute_loss(\n",
    "            network, \n",
    "            *generate_batch(val_img_codes, val_captions, batch_size)\n",
    "        )\n",
    "        val_loss += loss_t.item()\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    print('\\nЭпоха номер: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(network.state_dict(), checkpoint_path)\n",
    "        print('Сохранено')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq4lQY01-mVi"
   },
   "source": [
    "### Применение обученной модели \n",
    "\n",
    "На этом этапе мы попытаемся применить обученную сеть к своим изображениям. Для начала загрузим сеть, с использованием которой была выполнена векторизация изображений.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtRIzi0E-mVj"
   },
   "outputs": [],
   "source": [
    "vectorizer_model = <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH43EBPi-mVn"
   },
   "source": [
    "### Генерация описаний\n",
    "\n",
    "Реализуем функцию для генерации текстовых описаний по изображению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z6r0vWi-mVo"
   },
   "outputs": [],
   "source": [
    "def generate_caption(\n",
    "    image, \n",
    "    caption_prefix=(\"#START\"), \n",
    "    temp=1, \n",
    "    sample=True, \n",
    "    max_tokens=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерация описаний изображений.\n",
    "\n",
    "    Параметры.\n",
    "    1) image - изображение в формате RGB,\n",
    "    2) caption_prefix - начало сгенерированного текста,\n",
    "    3) temp - температура,\n",
    "    4) sample - при установке в True сэмплирует следующий токен на каждом этапе,\n",
    "    иначе - применяет жадный алгоритм,\n",
    "    5) max_tokens - максимальное количество токенов в итоговом описании.\n",
    "    \"\"\"\n",
    "  \n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >= 0 and image.shape[-1] == 3\n",
    "    \n",
    "    # отключим вычисление градиентов\n",
    "    with torch.no_grad():\n",
    "        # преобразуем изображение в тензор\n",
    "        image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "        vectors_neck = vectorizer_model(image[None])\n",
    "        caption_prefix = list(caption_prefix)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            prefix_ix = as_matrix([caption_prefix])\n",
    "            prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "            # вычисляем логиты и вероятности всех токенов словаря\n",
    "            next_word_logits = <...>\n",
    "            next_word_probs = <...>\n",
    "\n",
    "            assert len(next_word_probs.shape) == 1, 'вектор вероятностей должен быть одномерным'\n",
    "            \n",
    "            next_word_probs = <...> \n",
    "            # генерируем следующий токен описания\n",
    "            if sample:\n",
    "                next_word = <...>\n",
    "            else:\n",
    "                next_word = <...>\n",
    "                \n",
    "            # добавляем сгенерированный токен в ответ\n",
    "            caption_prefix.append(next_word)\n",
    "            \n",
    "            # если увидели токен конца текста, завершаем процесс\n",
    "            if next_word == \"#EOS\":\n",
    "                break\n",
    "            \n",
    "    return caption_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txug62ORdMZq"
   },
   "source": [
    "А теперь возьмите 5 любых изображений из интернета, желательно, сильно отличающихся друг от друга, и сгенерируйте для них описания с помощью обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCAeV8e9FUJM"
   },
   "outputs": [],
   "source": [
    "<...>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "combined_statement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
