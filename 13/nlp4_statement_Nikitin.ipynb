{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp4_statement_Nikitin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9d79345421c4682b9d5e8e70cf5975a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72b3a9b5d36d4b0f9ef97f39d376aaa0",
              "IPY_MODEL_1c73a9545c794a9db626221af7e695b6",
              "IPY_MODEL_fb84454df8f843b091e826272f0bcc48"
            ],
            "layout": "IPY_MODEL_3804af62e7d04b249eded7e7aebd12be"
          }
        },
        "72b3a9b5d36d4b0f9ef97f39d376aaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9088f6f9fc2437188df858c767993bc",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb95c50ca8c4627867cf82e0507625a",
            "value": "Downloading: 100%"
          }
        },
        "1c73a9545c794a9db626221af7e695b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c505c47beb432082436db7de447974",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9668667d5b1d4f2a8729f78dd9cc9f22",
            "value": 665
          }
        },
        "fb84454df8f843b091e826272f0bcc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca190681f73e4d5cbf5581665ce9e554",
            "placeholder": "​",
            "style": "IPY_MODEL_6f54d140fa7f45d79a469f579383af1d",
            "value": " 665/665 [00:00&lt;00:00, 15.0kB/s]"
          }
        },
        "3804af62e7d04b249eded7e7aebd12be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9088f6f9fc2437188df858c767993bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb95c50ca8c4627867cf82e0507625a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50c505c47beb432082436db7de447974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9668667d5b1d4f2a8729f78dd9cc9f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca190681f73e4d5cbf5581665ce9e554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f54d140fa7f45d79a469f579383af1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b461d1ccda24de78596205cb7e56619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56712febb78c4ce7b1f9fd18dcd834f0",
              "IPY_MODEL_14b10371089b4245a2e9f50854e934f2",
              "IPY_MODEL_0c5f41fc3ca24d8ca6fee1c887f6a104"
            ],
            "layout": "IPY_MODEL_67de2e962cc6400c85f222fb096a42cb"
          }
        },
        "56712febb78c4ce7b1f9fd18dcd834f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38e6e6823f7c4b798426d51b32f71137",
            "placeholder": "​",
            "style": "IPY_MODEL_65799d248bdd4b22974e8a16606dd6a4",
            "value": "Downloading: 100%"
          }
        },
        "14b10371089b4245a2e9f50854e934f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb84e1f6522f4582aa9509c777a8d440",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_907200dadcdf4209b94baa8bd626bb29",
            "value": 1042301
          }
        },
        "0c5f41fc3ca24d8ca6fee1c887f6a104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56c1c6a66b84997bee7618c5a82ff9c",
            "placeholder": "​",
            "style": "IPY_MODEL_82be76562ff84b0d906b7fd8080787af",
            "value": " 0.99M/0.99M [00:01&lt;00:00, 1.11MB/s]"
          }
        },
        "67de2e962cc6400c85f222fb096a42cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e6e6823f7c4b798426d51b32f71137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65799d248bdd4b22974e8a16606dd6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb84e1f6522f4582aa9509c777a8d440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907200dadcdf4209b94baa8bd626bb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d56c1c6a66b84997bee7618c5a82ff9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82be76562ff84b0d906b7fd8080787af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17dc456c6cfd4f81ad2a04361ad91b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60032d824a7a429789289902b3465413",
              "IPY_MODEL_900641b564f947a8bf493f9466f95eaf",
              "IPY_MODEL_8c1f466d67354464aeb401fd3be7b28a"
            ],
            "layout": "IPY_MODEL_6e3ce99761ba46edad7e63a437fc5298"
          }
        },
        "60032d824a7a429789289902b3465413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b270ac4740404db103f9d5a42017c0",
            "placeholder": "​",
            "style": "IPY_MODEL_96cfafe22e5c4dcb9faa076bd9b8d563",
            "value": "Downloading: 100%"
          }
        },
        "900641b564f947a8bf493f9466f95eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc6ba82ba304999ab50699b010ae17a",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5693a07d8af64ff0995d25895bba0c3f",
            "value": 456318
          }
        },
        "8c1f466d67354464aeb401fd3be7b28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a621f4edc1f74350a4fdaadc74582324",
            "placeholder": "​",
            "style": "IPY_MODEL_0e5bb37f83364391bcc5a14504a35310",
            "value": " 446k/446k [00:01&lt;00:00, 492kB/s]"
          }
        },
        "6e3ce99761ba46edad7e63a437fc5298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b270ac4740404db103f9d5a42017c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cfafe22e5c4dcb9faa076bd9b8d563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc6ba82ba304999ab50699b010ae17a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5693a07d8af64ff0995d25895bba0c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a621f4edc1f74350a4fdaadc74582324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e5bb37f83364391bcc5a14504a35310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef5c3c1091a64d638cb42b35276042f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4da4739a056479c900f5f35cc1cde04",
              "IPY_MODEL_ef34c276e4ce4da0b4d75ba43c11564b",
              "IPY_MODEL_1b25c6bd1e9f4a4285ce6e122d3bf46c"
            ],
            "layout": "IPY_MODEL_d7151aa74de0427e956f01a0e8f18cb7"
          }
        },
        "e4da4739a056479c900f5f35cc1cde04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae65628633364df3baabd6fb6578b6a0",
            "placeholder": "​",
            "style": "IPY_MODEL_dcc13643e1534cb5adc4884d6a3859ca",
            "value": "Downloading: 100%"
          }
        },
        "ef34c276e4ce4da0b4d75ba43c11564b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d38a1070bf864091bf296b94fc53f0a6",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1703e631d8934c58b46b44976fbda4cc",
            "value": 1355256
          }
        },
        "1b25c6bd1e9f4a4285ce6e122d3bf46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69988230154a4c6aad24ed23637f1713",
            "placeholder": "​",
            "style": "IPY_MODEL_035f62be948e40fb9fd5fa7b80fe4544",
            "value": " 1.29M/1.29M [00:01&lt;00:00, 1.08MB/s]"
          }
        },
        "d7151aa74de0427e956f01a0e8f18cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae65628633364df3baabd6fb6578b6a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc13643e1534cb5adc4884d6a3859ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d38a1070bf864091bf296b94fc53f0a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1703e631d8934c58b46b44976fbda4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69988230154a4c6aad24ed23637f1713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035f62be948e40fb9fd5fa7b80fe4544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1175c70b28574ceb97cf7189731d5f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a245c0656843babd70a9d7ea7efc4b",
              "IPY_MODEL_59f2c1fab79a4b4f899b27c4aae36c3b",
              "IPY_MODEL_8724e22d86824f14918ec0821fce4c14"
            ],
            "layout": "IPY_MODEL_4720b8e1ff56487b8e917ad98518e010"
          }
        },
        "62a245c0656843babd70a9d7ea7efc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e18de3a296b94961bc07678af3e6a1ef",
            "placeholder": "​",
            "style": "IPY_MODEL_1e5cadee173a4320907dc48bbfd8f912",
            "value": "Downloading: 100%"
          }
        },
        "59f2c1fab79a4b4f899b27c4aae36c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a184fe9c498244cfbe9ad34fc827e56b",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce7fd48d679a48519f4ed0ec68b9616b",
            "value": 548118077
          }
        },
        "8724e22d86824f14918ec0821fce4c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a604789b844f2482cdc67e401dc1fa",
            "placeholder": "​",
            "style": "IPY_MODEL_fac59bbd9acc49259709c1726d638f1c",
            "value": " 523M/523M [00:09&lt;00:00, 56.2MB/s]"
          }
        },
        "4720b8e1ff56487b8e917ad98518e010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18de3a296b94961bc07678af3e6a1ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e5cadee173a4320907dc48bbfd8f912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a184fe9c498244cfbe9ad34fc827e56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce7fd48d679a48519f4ed0ec68b9616b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13a604789b844f2482cdc67e401dc1fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac59bbd9acc49259709c1726d638f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dPfVKv2adDm"
      },
      "source": [
        "# Машинное обучение, DS-поток\n",
        "## Задание 1.13\n",
        "\n",
        "\n",
        "**Правила:**\n",
        "\n",
        "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ad21_bot`.\n",
        "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
        "* Прислать нужно ноутбук в формате `ipynb`.\n",
        "* Решения, размещенные на каких-либо интернет-ресурсах не принимаются. Публикация решения может быть приравнена к предоставлении возможности списать.\n",
        "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него.\n",
        "\n",
        "---\n",
        "\n",
        "## Генерация текстов с трансформеров.\n",
        "\n",
        "**За задание можно получить 6 баллов.** \n",
        "\n",
        "В данном задании вы будете вновь генерировать тексты на основании высказываний [Ницше](https://ru.wikipedia.org/wiki/Ницше,_Фридрих). \n",
        "Ваша задача использовать трансформеры для генерации текстов.\n",
        "\n",
        "Попробуем улучшить результат по сравнению с реккурентными сетями.\n",
        "\n",
        "В задании можно использовать как свои, так и предобученные сети\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGOV69fU0CR9",
        "outputId": "192c6860-72ff-4af6-ab78-21a24742b6b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 30.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=be142ee3b7e3edd09720b258f31fca50791112beea3ca0d8c388ba7ed6cf104c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xt4DweWs4RO"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import LineByLineTextDataset,TextDataset,\\\n",
        " DataCollatorForLanguageModeling,Trainer, TrainingArguments\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "RANDOM_SEED = 17"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qED98FUNs_ej"
      },
      "source": [
        "Скачиваем данные. Корпус состоит из высказываний Ницше, разделеных переносом строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxtkObdPtEd1",
        "outputId": "a68cdbb5-f45d-4749-a94e-798f835abc24"
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1BTyKYpAQ0yLBqHVMQ_12BsLJVyixgp9a' -O nietzsche.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-12 08:24:21--  https://docs.google.com/uc?export=download&id=1BTyKYpAQ0yLBqHVMQ_12BsLJVyixgp9a\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.12.102, 142.251.12.113, 142.251.12.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.12.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-4c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5n0bd4qjdf64bulpb4hl7hbtf58cnv1p/1652343825000/14359032242157329066/*/1BTyKYpAQ0yLBqHVMQ_12BsLJVyixgp9a?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-12 08:24:23--  https://doc-14-4c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5n0bd4qjdf64bulpb4hl7hbtf58cnv1p/1652343825000/14359032242157329066/*/1BTyKYpAQ0yLBqHVMQ_12BsLJVyixgp9a?e=download\n",
            "Resolving doc-14-4c-docs.googleusercontent.com (doc-14-4c-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-14-4c-docs.googleusercontent.com (doc-14-4c-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: ‘nietzsche.txt’\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-05-12 08:24:24 (155 MB/s) - ‘nietzsche.txt’ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ9QYAI7XFHX",
        "outputId": "44003c83-9fc8-4694-a88b-71c8bd820fe9"
      },
      "source": [
        "!head nietzsche.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE\n",
            "\n",
            "\n",
            "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
            "for suspecting that all philosophers, in so far as they have been\n",
            "dogmatists, have failed to understand women--that the terrible\n",
            "seriousness and clumsy importunity with which they have usually paid\n",
            "their addresses to Truth, have been unskilled and unseemly methods for\n",
            "winning a woman? Certainly she has never allowed herself to be won; and\n",
            "at present every kind of dogma stands with sad and discouraged mien--IF,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with io.open('./nietzsche.txt', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "data = text.split('\\n')"
      ],
      "metadata": {
        "id": "c-z1frixgdf0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=0\n",
        "all_len=[]\n",
        "for line in data:\n",
        "    max_len=max(max_len,len(line))\n",
        "    all_len.append(len(line))\n",
        "max_len, np.median(all_len),len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAFnN7cXvBOu",
        "outputId": "5232ff5b-13fa-49f7-e9e7-a18dda55469e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 68.0, 9935)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "tokens = list(set(tokenizer.tokenize(text)))\n",
        "print('total tokens:', len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-ELFz1tvHYs",
        "outputId": "186d19e5-6770-4f69-ab57-abd96608ac7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total tokens: 11393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokens = list(set(tokenizer.tokenize(text)))\n",
        "print('total tokens:', len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "d9d79345421c4682b9d5e8e70cf5975a",
            "72b3a9b5d36d4b0f9ef97f39d376aaa0",
            "1c73a9545c794a9db626221af7e695b6",
            "fb84454df8f843b091e826272f0bcc48",
            "3804af62e7d04b249eded7e7aebd12be",
            "c9088f6f9fc2437188df858c767993bc",
            "cfb95c50ca8c4627867cf82e0507625a",
            "50c505c47beb432082436db7de447974",
            "9668667d5b1d4f2a8729f78dd9cc9f22",
            "ca190681f73e4d5cbf5581665ce9e554",
            "6f54d140fa7f45d79a469f579383af1d",
            "4b461d1ccda24de78596205cb7e56619",
            "56712febb78c4ce7b1f9fd18dcd834f0",
            "14b10371089b4245a2e9f50854e934f2",
            "0c5f41fc3ca24d8ca6fee1c887f6a104",
            "67de2e962cc6400c85f222fb096a42cb",
            "38e6e6823f7c4b798426d51b32f71137",
            "65799d248bdd4b22974e8a16606dd6a4",
            "fb84e1f6522f4582aa9509c777a8d440",
            "907200dadcdf4209b94baa8bd626bb29",
            "d56c1c6a66b84997bee7618c5a82ff9c",
            "82be76562ff84b0d906b7fd8080787af",
            "17dc456c6cfd4f81ad2a04361ad91b28",
            "60032d824a7a429789289902b3465413",
            "900641b564f947a8bf493f9466f95eaf",
            "8c1f466d67354464aeb401fd3be7b28a",
            "6e3ce99761ba46edad7e63a437fc5298",
            "09b270ac4740404db103f9d5a42017c0",
            "96cfafe22e5c4dcb9faa076bd9b8d563",
            "1dc6ba82ba304999ab50699b010ae17a",
            "5693a07d8af64ff0995d25895bba0c3f",
            "a621f4edc1f74350a4fdaadc74582324",
            "0e5bb37f83364391bcc5a14504a35310",
            "ef5c3c1091a64d638cb42b35276042f3",
            "e4da4739a056479c900f5f35cc1cde04",
            "ef34c276e4ce4da0b4d75ba43c11564b",
            "1b25c6bd1e9f4a4285ce6e122d3bf46c",
            "d7151aa74de0427e956f01a0e8f18cb7",
            "ae65628633364df3baabd6fb6578b6a0",
            "dcc13643e1534cb5adc4884d6a3859ca",
            "d38a1070bf864091bf296b94fc53f0a6",
            "1703e631d8934c58b46b44976fbda4cc",
            "69988230154a4c6aad24ed23637f1713",
            "035f62be948e40fb9fd5fa7b80fe4544"
          ]
        },
        "id": "t6lZZTQ7zpAR",
        "outputId": "e6e7dfe4-6dd2-44d8-f9e5-aa478cade9d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d79345421c4682b9d5e8e70cf5975a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b461d1ccda24de78596205cb7e56619"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17dc456c6cfd4f81ad2a04361ad91b28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5c3c1091a64d638cb42b35276042f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (143018 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total tokens: 10680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предлагаю прочитать вот данную [статью](https://huggingface.co/docs/transformers/tokenizer_summary). В ней говорится, что в GPT-2 используется BPE"
      ],
      "metadata": {
        "id": "epkYUFTe34eT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обосную почему я необходимо использовать именно gpt. Еще на самом деле у нас есть bert, но на офиицвльном сайте сказано \"BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\" Так что для генерации текста лучше всего подойдет gpt. "
      ],
      "metadata": {
        "id": "JAlA1yPzaeAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "1175c70b28574ceb97cf7189731d5f6b",
            "62a245c0656843babd70a9d7ea7efc4b",
            "59f2c1fab79a4b4f899b27c4aae36c3b",
            "8724e22d86824f14918ec0821fce4c14",
            "4720b8e1ff56487b8e917ad98518e010",
            "e18de3a296b94961bc07678af3e6a1ef",
            "1e5cadee173a4320907dc48bbfd8f912",
            "a184fe9c498244cfbe9ad34fc827e56b",
            "ce7fd48d679a48519f4ed0ec68b9616b",
            "13a604789b844f2482cdc67e401dc1fa",
            "fac59bbd9acc49259709c1726d638f1c"
          ]
        },
        "id": "kz4IGFpy3uzW",
        "outputId": "82a857e4-8c6f-45e3-f68b-98fad2a45627"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1175c70b28574ceb97cf7189731d5f6b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_texts(model, prefix, num_texts=5, max_length=100):\n",
        "    '''\n",
        "    Функция для генерации текстов с помощью GPT \n",
        "\n",
        "    Параметры.\n",
        "    1) model — модель, с помощью которой осуществляется генерация, \n",
        "    2) prefix — начало текста,\n",
        "    3) num_texts — количество текстов, которые требуется сгенерировать,\n",
        "    4) max_length — максимальное количество добавляемых токенов.\n",
        "    \n",
        "    '''\n",
        "\n",
        "    for _ in range(num_texts):\n",
        "        print(\"_\" * 50)\n",
        "        tensor_prefix = torch.tensor([tokenizer.encode(prefix)]).to(device)\n",
        "        generated_tokens = model.generate(\n",
        "            tensor_prefix, max_length=max_length, do_sample=True, top_k=42, temperature=0.9, verbose=False\n",
        "        )\n",
        "        decoded_text = tokenizer.decode(generated_tokens[0])\n",
        "        print(decoded_text)"
      ],
      "metadata": {
        "id": "nRqldGYtXQck"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# избавимся от избыточного логирования\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "i9V1gRfrZjsV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_texts(model, \"Hi, I'm Vladimir\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IkLsyjSXtfL",
        "outputId": "ebe359f4-fce6-4fad-a09a-342b013e2612"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "Hi, I'm Vladimir. It's a great pleasure to welcome you. Welcome back to this show\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Putin.\n",
            "\n",
            "If there are any of you who've been following the\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir. I'm a developer, who helps people use their mobile devices in a\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir. I've been here since the beginning, but now I'm starting my\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Ilyich Lenin! You're listening to The Lenin Show on Radio Free Europe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что довольно обдуманный текст, но давайте все же приступим к нашему заданию. Хорошо видно, что так как он обучался на большом количестве английской литературы, то он запомнил самых известных Владимиров (Путин и Ленин). Для этого будем использовать готовые функции для удобства. У трансформера есть свой реализованный [train](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.model). Это довольно сильно облегчит написание кода"
      ],
      "metadata": {
        "id": "Sqkyf41qt2wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но на самом для нас самая большая проблема это создать датасет, все остальное должно делать довольно легко. Есть прекрасный класс TextDataset, вот [тут](https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/language_modeling.py) больше информации есть. Еще одной идеей было использовать LineByLineTextDataset, но есть нюанс, что изначально gtp обучали без pad, то есть такого символа просто нет в токенизации, поэтому будет вылезать ошибка. Я нашел такое [обсуждение](https://github.com/huggingface/transformers/issues/4122) данной проблемы. В таком случае лучше использовать TextDataset, чем LineByLineTextDataset."
      ],
      "metadata": {
        "id": "yCvj0UZj2KdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "block_size=int(max_len*2)\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path='./nietzsche.txt',block_size=block_size)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmJCBHWpU0ir",
        "outputId": "0aa04252-1485-400f-c3bc-5e75f937a601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (143770 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)\n",
        "model.train(True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./history_model\", \n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    disable_tqdm=False,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgDkMBA_vQIw",
        "outputId": "f477a746-c11f-411e-9b3f-0ce07ecb49df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gpu():\n",
        "  global model,training_args,trainer\n",
        "  del model,training_args,trainer\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zWZIfOZp4C2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_50_epochs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "ybNnN3P09tf7",
        "outputId": "35ecb3e4-9bd6-455b-e142-fa1ad36b5dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 1:00:28, Epoch 49/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.454400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.186500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.170800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_100_epochs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "HRBWMzvrd_Xl",
        "outputId": "b786d47c-a457-4c46-80eb-f34b93330fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='103' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [103/350 17:29 < 42:45, 0.10 it/s, Epoch 14.51/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.170600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-350] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-213bd26bb8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gpt_model_100_epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m                 ):\n\u001b[1;32m   1429\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошо видно, что лосс улучшается уже почти незаметно, так что можно остановиться"
      ],
      "metadata": {
        "id": "f6uxhyIRiF4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "logging.set_verbosity_error()\n",
        "generate_texts(model, 'people love ml and', max_length=50)"
      ],
      "metadata": {
        "id": "66iuqG0AD629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def8aa1a-93bb-4667-e0f9-d4560583f4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "people love ml and their own kind. But the man who has never hitherto, at any rate, had the means to know when he was right\n",
            "when he had been wrong, the man who has always had the means of being right and\n",
            "never\n",
            "__________________________________________________\n",
            "people love ml and h to know how\n",
            "they are not a burden to the world or their environment; the truth is, they are\n",
            "mere means of obtaining the knowledge of an immense and\n",
            "profoundly powerful world of which there is no small\n",
            "__________________________________________________\n",
            "people love ml and m. The only reason why people do such things is they hate the thing\n",
            "they hate--it is only they who, after all, love and enjoy it. There is no place for\n",
            "that; it is only they who\n",
            "__________________________________________________\n",
            "people love ml and pf men,\n",
            "the \"glamorous\" man, the \"sweet,\" and the \"unconditional,\" may enjoy the same\n",
            "special relationship with the feminine. In speaking of \"love,\" what is it\n",
            "not\n",
            "__________________________________________________\n",
            "people love ml and man, and\n",
            "love the love of him who looks upon him as a father.\n",
            "He does not appreciate that nature, as it were, is a sort of\n",
            "dissertation; yet, as the philosopher, he is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " generate_texts(model,  \"artificial intelegence is interesting\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJEgJn75icvN",
        "outputId": "63314b58-7554-47c2-c23c-189e1adce44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "artificial intelegence is interesting and difficult to reconcile with what seems to be the fact of an \"emotionally\" connected state of the soul. The first step in the explanation of the fact of \"emotions\"--inasmuch\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting, if\n",
            "it may be that, in the minds of a great multitude of philosophers,\n",
            "a great\n",
            "intellect, a superintellect and a superdeterioration of the will and action,\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting enough\n",
            "even to the extent that it is generally perceived that the more\n",
            "intelligent one is, the more difficult it is for the soul to deal with the \"intelligences\n",
            "of which it is so\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to consider, and\n",
            "will not be allowed any more.\n",
            "\n",
            "\n",
            "59\n",
            "\n",
            "=How to Make Sense of a Man's World.=--I have hitherto treated of man's\n",
            "world, according to\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting, that every man, in effect, is the master of a certain\n",
            "form of knowledge and the master. But to the extent that a man has ever attempted\n",
            "to build any knowledge on the grounds that it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " generate_texts(model,  \"be no mistake about it:\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5chxs102ikgN",
        "outputId": "f5a0ae94-6785-420d-9bc4-ca6efb3fbee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "be no mistake about it:\n",
            "its existence, the nature of its subject, the \"ultimate power\"\n",
            "of its object, its origin, which the reader has in mind, may be\n",
            "a delusion, an error in judgment, such as a\n",
            "__________________________________________________\n",
            "be no mistake about it: the whole process\n",
            "should be the responsibility of the individual, and should therefore be undertaken\n",
            "according to the requirements set forth in the law.\n",
            "\n",
            "234. In the event of a German revolt, the majority of Germans\n",
            "__________________________________________________\n",
            "be no mistake about it: the present has a new origin; the whole\n",
            "origin of life has been acquired--an origin which is now far greater than before. What\n",
            "has happened to the people of Europe since the beginning has\n",
            "been the result\n",
            "__________________________________________________\n",
            "be no mistake about it:\n",
            "It is the most delicate, unbreakable rule for all living beings\n",
            "to maintain in mind that, whatever their own power seems to\n",
            "be, even one cannot always say to them: \"Here in that order\n",
            "__________________________________________________\n",
            "be no mistake about it: a German\n",
            "man who has spent his whole life in the struggle for his own and also for others' right\n",
            "to live\n",
            "must do as best he can, for in the end he is still free. Here are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получается связный текст, но нет ощущения, что это философский текст, так как наша модель обучалась очень много эпох и на больших других данных, а тут мы совсем немного поучили и на маленьких данных."
      ],
      "metadata": {
        "id": "Sl_fcWXh-CmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте попробуем с другим lr, начнем с более большим"
      ],
      "metadata": {
        "id": "3j2TKYcQi8bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)\n",
        "model.train(True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./history_model\", \n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    disable_tqdm=False,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-4), None)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2X5POPji7zX",
        "outputId": "87e5ca01-29e3-4801-d967-cf09dcc241aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_lr_-4_50_epochs_')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "R27nTMYijQs4",
        "outputId": "0fe2c3d8-7ad1-4792-d486-09a49d95a663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 1:00:32, Epoch 49/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.243500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.671700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.283800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.472800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_lr_-4_epochs_100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "coASLDPDxmam",
        "outputId": "8c560ef3-714f-4449-f51e-c469b7dfdbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='349' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [349/350 1:00:33 < 00:10, 0.10 it/s, Epoch 49.63/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.454600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.456200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.457000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.454300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.458000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-350] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте попробуем уменьшить lr"
      ],
      "metadata": {
        "id": "9b6rPxVtWZyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('gpt_model_lr_-4_epochs_100')\n",
        "model.train(True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./history_model\", \n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    disable_tqdm=False,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
        ")"
      ],
      "metadata": {
        "id": "Cd0thMqAWhMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_lr_-4_to_-5_epochs_150')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "OFUYqApXWc-6",
        "outputId": "85f214b5-e1f2-4a7d-a239-a9c892b334c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 1:02:52, Epoch 49/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.333200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.263600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.246100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.282400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "logging.set_verbosity_error()\n",
        "generate_texts(model, 'people love ml and', max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"artificial intelegence is interesting\", max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"be no mistake about it:\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAC3WjvbvWBl",
        "outputId": "bb64e3f6-0658-4ba2-bb00-33ccef60f118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "people love ml and no\n",
            "more, for they are afraid of losing their taste. \"Everything is too\n",
            "imperfect\"--they always tell themselves after many pokes and lacerations, at\n",
            "which, indeed, many a mediocre man will\n",
            "__________________________________________________\n",
            "people love ml and know how to\n",
            "betray themselves--because in all honesty, it turns out that in _everything_\n",
            "the love of a thing_ it is _absolutely_ NOT the love of a God\n",
            "and _absolutely NOT the love\n",
            "__________________________________________________\n",
            "people love ml and men,\"\n",
            "--they must exchange for each other the worst instincts of\n",
            "aggression and hate, hatred and modesty.\n",
            "\n",
            "271. In a certain period of the Russian Empire Christians really\n",
            "did not believe as they did that\n",
            "__________________________________________________\n",
            "people love ml and hate it! But ye must learn to love it also, and for ever\n",
            "more\n",
            "love it!\"\n",
            "\n",
            "212. \"It gives me joy, it makes me sad, it puts all my worries into\n",
            "order, and\n",
            "__________________________________________________\n",
            "people love ml and know how to gratefully and with sincere severity, who withal\n",
            "have at last persuaded themselves to call life almighty, that good,\n",
            "mighty life, that for every morsel of sin there is a BODY\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting and quite possibly\n",
            "a sign of deep, emotional man--to feel oneself once more elevated\n",
            "by this immense spirituality and its profound excellency, and by behold\n",
            "these labyrinthine vases and labyrinthine\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting, too, both because\n",
            "it is pleasing to one degree, and because it brings laughter to another\n",
            "degree. It may be necessary, as is usual in cases, to pose the question\n",
            "to the utmost\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting\n",
            "to men because they are annoyed with philosophers who are too rigid and\n",
            "irrational to understand them. The philosophical instinct forces them to reflect\n",
            "on themselves, and to look around in different ways, and not\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting in a certain\n",
            "sense, because it seems to command respectability and credibility even among the\n",
            "hard-hearted and self-contradictory. I will venture to express in more\n",
            "depth what I have\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting in\n",
            "the present tense and in any tense in which it may be understood. I mean to say\n",
            "that in a sentence, and with reference to itself, the intentional\n",
            "introversion of meaning is as\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "be no mistake about it: he is precisely\n",
            "an ape--an ape who CANNOT learn to be moral; in fact, he is\n",
            "perhaps an ape who CANNOT learn to be good.\" One has, in\n",
            "short, to go through\n",
            "__________________________________________________\n",
            "be no mistake about it: the more\n",
            "dangerous philosophers nowadays are, the more likely they are to make mistakes in\n",
            "the long run; the more difficult it is for them to gain the upper hand\n",
            "with regard to their work, the higher\n",
            "__________________________________________________\n",
            "be no mistake about it: in the end, in all honesty\n",
            "about it, there is NO NECESSITY IN PERVERSITY: it always appears\n",
            "with the utmost solemnity that all morality is vanity, an afterthought,\n",
            "which,\n",
            "__________________________________________________\n",
            "be no mistake about it: the\n",
            "old dogmatic suspicion of causation was not a basis for the\n",
            "secrecy with which Spinoza and his followers were treated in their\n",
            "congratulates and monologues, but was rather a means and\n",
            "__________________________________________________\n",
            "be no mistake about it: the Chinese enjoy a\n",
            "better, further-sighted, and more complementary industrialization, and want\n",
            "something which will appease their foreign and amorous inclinations, by\n",
            "designating the South as their common share in the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст стал более философским, причем некоторые фразы очень философские, особенно \"be no mistake about it: the more\n",
        "dangerous philosophers nowadays are, the more likely they are to make mistakes in\n",
        "the long run;\", \"не заблуждайтесь на этот счет: он именно\n",
        "обезьяна - обезьяна, которая НЕ МОЖЕТ научиться быть нравственной; на самом деле, он, возможно, обезьяна, которая НЕ МОЖЕТ научиться быть хорошей\"."
      ],
      "metadata": {
        "id": "vjt8PAC1DPiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте попробуем с lr=1e-3 с scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
      ],
      "metadata": {
        "id": "-bOXJQ-dXZPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)\n",
        "model.train(True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./history_model\", \n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    disable_tqdm=False,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (optimizer, torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=16*8))\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4bnvx7yv986",
        "outputId": "6cf2fa8c-0907-49eb-de5e-067ca1731473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_CosineAnnealingLR_lr=1e-3_epochs_50')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "_rd0mRbvw7jU",
        "outputId": "7c3eee00-8430-4a69-f75e-36d42bf49e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 58:22, Epoch 49/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.765900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.273800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.143000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.063200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим мы получили потрясающий результат метрики намного лучше, но давайте попробуем еще дообучить"
      ],
      "metadata": {
        "id": "nnWEBP_xItmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_CosineAnnealingLR_lr=1e-3_epochs_100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "AVuVCsSvW1Jd",
        "outputId": "1963fbf7-2c47-472a-abac-47c605ad3996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 50\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 58:19, Epoch 49/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.041800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.043500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.051700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.042200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.030300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.029100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-350] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как хорошо видно, метрика уже не так сильно улучшается, давайте протестируем на генерации текста"
      ],
      "metadata": {
        "id": "4uLo820sLxx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "logging.set_verbosity_error()\n",
        "generate_texts(model, 'people love ml and', max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"artificial intelegence is interesting\", max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"be no mistake about it:\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzAUfgVPnomD",
        "outputId": "28aca0a9-05f2-4090-f677-1b4f59417c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "people love ml and play; I rather think that as the love\n",
            "of mankind grows, the more firmly and forever chained to its place\n",
            "in the heart of mankind, the more polyphone the music and the sounding of its\n",
            "effects, the more\n",
            "__________________________________________________\n",
            "people love ml and play; it is strange to them if they do not\n",
            " love one another!\n",
            "\n",
            "51. In praise there is more obtrusiveness than in blame.\n",
            "\n",
            "52. What a person IS begins to betray itself when\n",
            "__________________________________________________\n",
            "people love ml and plays THIS bad\n",
            "game.\n",
            "\n",
            "206. In relation to the genius, that is to say, a being who either\n",
            "ENGENDERS or PRODUCES--both words understood in their fullest sense--the\n",
            "man\n",
            "__________________________________________________\n",
            "people love ml and play; I even\n",
            "themselves do not know what they are, and what they serve us\n",
            "as a good conscience. But, as has been pointed out,\n",
            "this is a realm of questions and answers in which a more\n",
            "__________________________________________________\n",
            "people love ml and thinking; whoever has thoroughly realized how absurdly false and\n",
            "sentimental this proposition is, in a world whose essence is Will\n",
            "to Power, may be reminded that Schopenhauer, although a pessimist,\n",
            "ACTU\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to philosophers, because it\n",
            "is so often enough misinterpreted and misinterpreted in the same manner\n",
            "that the German who says \"I\" means \"I\" rather than \"I\"\n",
            "desires a different\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to philosophers, as\n",
            "their HIGHEST rank entails something touching,\n",
            "almost TURNING, ordinary, and fundamentally different; the\n",
            "interminable tension of soul in such a case is of profoundest\n",
            "\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to us, let us look for it in our\n",
            "\"interests\"--and not we even in our \"free-spirited\" inclinations! That\n",
            "these last words may not be misunderstood, that\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to people all ages,\n",
            "even to philosophers; they are both hard and clumsy at\n",
            "all times; in the end, however, one finds\n",
            "themselves even in the worst and most dangerous sense of\n",
            "\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to people all\n",
            "the same as if it were a play, something childish and\n",
            "threatening, to teach and to inspire sympathy; the\n",
            "interrogation mark of the genius is uncanny. In what uncanny anticip\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "be no mistake about it: in fact, the vice of the\n",
            "question, as to why knowledge at all costs should be so\n",
            "refined and fast, and in any case the instinct of the\n",
            "reviving sciences should be very much on its\n",
            "__________________________________________________\n",
            "be no mistake about it: in fact, the vice of the\n",
            "question, as regards the form and substance (Werthschätzungen) of\n",
            "this morality, is the same mistake that has been made also about the\n",
            "former\n",
            "__________________________________________________\n",
            "be no mistake about it: in fact, a man who has errors of\n",
            "every kind, and consequently of the dangerous formula, \"God on the Cross,\" has hitherto been the greatest\n",
            "mistake of all philosophers. Inasmuch as in all\n",
            "__________________________________________________\n",
            "be no mistake about it: in the world of historical values\n",
            "it is always more difficult to learn what WAS EXPECTED, and also to\n",
            "perceive what was EXPECTED; with regard to the NEW conception\n",
            "of \"history\"--\n",
            "__________________________________________________\n",
            "be no mistake about it: in the world of historical values\n",
            "spurious coinage PREVAILS. Those great poets, for example, such as\n",
            "Byron, Musset, Poe, Leopardi, Kleist, Gogol (I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получили более философский текст, но есть местами неидеально связный текст, скорее всего, это связано с тем, что мы начали сильно переобучаться под определенный текст, но давайте посмотрим, что покажет данная модель на следующем примере."
      ],
      "metadata": {
        "id": "fd-R_choNWb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_texts(model, \"Hi, I'm Vladimir\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4unhOPln1GS",
        "outputId": "db3231e2-7bc1-42c9-faec-eed117f07e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "Hi, I'm Vladimir--I hope you understand\n",
            "that I am not a skeptic. I\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir--I know you well enough, my friends. It\n",
            "is the appearance\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir--I know that already.\n",
            "\n",
            "220. I may be mistaken about\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir--I know you well enough how to help me!\n",
            "with a good\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir, and I know that\n",
            "there is a second, also, by means\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi, I'm Vladimir--I hope you understand\n",
        "that I am not a skeptic. Оч смешная и философская цитата получилась, очень показательно если сравнить с генерацией в начале задания только на предобученной модели, то он учитывал мировых политических лидеров (известных людей с именем Владимир). Теперь же он генерирует философский текст, для человека с именем Владимир."
      ],
      "metadata": {
        "id": "Vz6S9eOlV-wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте попробуем теперть повторить предыдущий пункт, но с более маленьким lr."
      ],
      "metadata": {
        "id": "mt5x_F85cXtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(device)\n",
        "model.train(True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./history_model\", \n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=100,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    disable_tqdm=False,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    )\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (optimizer, torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=16*8))\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1202fc-a1ae-44fe-8e3a-d1ffcc1bcd90",
        "id": "eHLbTC2PoLuB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "torch.save(model, 'gpt_model_CosineAnnealingLR_lr=1e-4_epochs_100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5da8f8c-1e16-412f-913c-5a1218773c01",
        "id": "Vnle4zOcoLuD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 998\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 700\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [700/700 1:56:56, Epoch 99/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.244900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.755500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.592600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.962900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.649600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.546900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.222200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.877500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.543000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.336300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./history_model/checkpoint-50\n",
            "Configuration saved in ./history_model/checkpoint-50/config.json\n",
            "Model weights saved in ./history_model/checkpoint-50/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-100\n",
            "Configuration saved in ./history_model/checkpoint-100/config.json\n",
            "Model weights saved in ./history_model/checkpoint-100/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-350] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-150\n",
            "Configuration saved in ./history_model/checkpoint-150/config.json\n",
            "Model weights saved in ./history_model/checkpoint-150/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-50] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-200\n",
            "Configuration saved in ./history_model/checkpoint-200/config.json\n",
            "Model weights saved in ./history_model/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-250\n",
            "Configuration saved in ./history_model/checkpoint-250/config.json\n",
            "Model weights saved in ./history_model/checkpoint-250/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-150] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-300\n",
            "Configuration saved in ./history_model/checkpoint-300/config.json\n",
            "Model weights saved in ./history_model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-350\n",
            "Configuration saved in ./history_model/checkpoint-350/config.json\n",
            "Model weights saved in ./history_model/checkpoint-350/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-250] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-400\n",
            "Configuration saved in ./history_model/checkpoint-400/config.json\n",
            "Model weights saved in ./history_model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-450\n",
            "Configuration saved in ./history_model/checkpoint-450/config.json\n",
            "Model weights saved in ./history_model/checkpoint-450/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-350] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-500\n",
            "Configuration saved in ./history_model/checkpoint-500/config.json\n",
            "Model weights saved in ./history_model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-400] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-550\n",
            "Configuration saved in ./history_model/checkpoint-550/config.json\n",
            "Model weights saved in ./history_model/checkpoint-550/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-450] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-600\n",
            "Configuration saved in ./history_model/checkpoint-600/config.json\n",
            "Model weights saved in ./history_model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-650\n",
            "Configuration saved in ./history_model/checkpoint-650/config.json\n",
            "Model weights saved in ./history_model/checkpoint-650/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-550] due to args.save_total_limit\n",
            "Saving model checkpoint to ./history_model/checkpoint-700\n",
            "Configuration saved in ./history_model/checkpoint-700/config.json\n",
            "Model weights saved in ./history_model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [history_model/checkpoint-600] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы получили довольно хороший лосс по итогу, хотя он намного больше, чем когда мы пробовали с lr=1e-3"
      ],
      "metadata": {
        "id": "pLBsajLWdlMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "logging.set_verbosity_error()\n",
        "generate_texts(model, 'people love ml and', max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"artificial intelegence is interesting\", max_length=50)\n",
        "print(\"_\" * 50)\n",
        "generate_texts(model,  \"be no mistake about it:\", max_length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829afc8f-ba56-40ad-8180-1354874d69eb",
        "id": "Zxhd2RHnEfKq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "people love ml and romania more than we love the\n",
            "martyr,\"--this love has an exceedingly rapid growth, for the very reason\n",
            "that on the one hand the plebeian and plebeian eye has long ago\n",
            "looked\n",
            "__________________________________________________\n",
            "people love ml and m\n",
            "in me! What! A man of science who has failed to see that science\n",
            "himself is a MIRROR--PLATONISM! Oh, gosh! Such an INDIFFERENCE of\n",
            "things\n",
            "__________________________________________________\n",
            "people love ml and loon.\n",
            "\n",
            "201. What!? A philosopher? Nay, only a poet? But you misunderstand me--when\n",
            "YOU attach so much importance to a philosopher, you are only addressing\n",
            "a personal attack on him--name\n",
            "__________________________________________________\n",
            "people love ml and nothing else. There may even be\n",
            "more in them, hidden under the skin, mingled with fishnet threadbare feet and\n",
            "hair, like fine beads of silver. But this is neither here, nor\n",
            "there anything else\n",
            "__________________________________________________\n",
            "people love ml and nothing else, the noble\n",
            "individuals of all nations strive with one another for supremacy,\n",
            "inasmuch as the common necessities lead to the conclusion that\n",
            "they must be fed and clothed. Every noble man feels himself, in\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting also to US taste, as a\n",
            "religio-religious TEMPO by way of which he draws his conclusions.]--It is\n",
            "the strangest thing I know about myself; but I know nevertheless\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to US taste at present, perhaps as a\n",
            "\"gold standard\" somewhere, perhaps as the \"Voltaire of morality,\"\n",
            "or as Shakespeare in \"Hamlet.\" The artificial, the \"ill\"\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting to US tastes even in the\n",
            "far reaches of our souls. For instance, if I pursue a long straight line, I always\n",
            "feel a sort of \"downward movement,\" as if I were plummeting\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting in many cases, because it seems to\n",
            "offer an opportunity for confession, a small token of esteem, perhaps\n",
            "of authority--or even a binding, involuntary repudiation of one's\n",
            "ancient conceptions of\n",
            "__________________________________________________\n",
            "artificial intelegence is interesting, although it is very artificial and\n",
            "a little coarse-mannered: that artificiality may not deceive itself, at least for a time\n",
            "it will be \"immediately known\". At present people are sufficiently\n",
            "__________________________________________________\n",
            "__________________________________________________\n",
            "be no mistake about it: \"We are not hostile to those who do not like our good\n",
            "taste, and there are other good reasons for opposing our bad\n",
            "taste\";--supposing one accepts the proposition honestly, with an\n",
            "est\n",
            "__________________________________________________\n",
            "be no mistake about it: the more a philosopher women\n",
            "he learns about herself, the more she must treat man and\n",
            "woman alike--they must be fundamentally different creatures.\n",
            "\n",
            "\n",
            "72\n",
            "\n",
            "=Possibility of Progress.=--Man and\n",
            "__________________________________________________\n",
            "be no mistake about it: a noble\n",
            "practice, perhaps noble self-control and circumspection therein, when the occasion\n",
            "is sufficiently great, when vanity itself is being sought and the most\n",
            "important thing therein attained; perhaps a more subtle and more\n",
            "__________________________________________________\n",
            "be no mistake about it: a German needs an empire even\n",
            "more! He needs an empire of his own, and moreover a second and further\n",
            "subjugation, above all a stronger, further, and further ethnological empire,\n",
            "a \"\n",
            "__________________________________________________\n",
            "be no mistake about it: Christianity, the greatest\n",
            "poets of our time, was in reality a protest against PHILOSOPHERS--and\n",
            "anticipating a new atheism. For the philosophers of today are almost\n",
            "instinctively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_texts(model, \"Hi, I'm Vladimir\", max_length=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f90fd2-27f5-4229-8251-849c31bee80c",
        "id": "H1njwqnKEfKr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________________________________________________\n",
            "Hi, I'm Vladimir Putin.)\n",
            "\n",
            "236. The Russian Empire as a Continent?\n",
            "\n",
            "\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Putin!\"\n",
            "\n",
            "189. He who attains his ideal, gets along\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Nabokov, and now I'm playing the\n",
            "secretary of state\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Nabokov!\" And thus\n",
            "the great, all powerful, all historical\n",
            "__________________________________________________\n",
            "Hi, I'm Vladimir Putin,\" he says,\n",
            "on the contrary of all the vivis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут очень показательно, что с именем Владимир он все еще предпочитает генерировать текст из статей, на которых он обучался (то есть он упоминает известных людей с именем Владимир)"
      ],
      "metadata": {
        "id": "SEDHdu4ld2jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:** Сначала скажу про токенизацию. Как говорилось раньше нам необходима BPE, так как это более оптимально для связной и хорошей генерации текста. Но в gpt2 использует даже более хитрую вещь, а именно BPE на уровне байтов. Также я перебрал различные lr и с/без schedule. Лучшие результаты показал lr=1e-3 с schedule. Именно он дал результат на моей цитате не похожий на результат \"голой\" модели, то есть он отдает предпочтению текстам ницше, а не предобученной моделью. Все остальные модели цитату \"Hi, I'm Vladimir\" продолжают используя фамилии известных людей, а не начинают философски излагать мысль. Но при этом возможно что самая лучшая модель начинает переобучаться под данный текст, и немного терять логичность и связность в высказывании. Также хотелось бы проговорить почему было неактуально с 0 обучать нашу модель. Для обучения трансформеров используют очень большое кол-во данных, то есть для gpt2 использовали порядка 40 gb. Так что было нецелесообразно обучать с 0 нашу модель на наших маленьких даннных, скорее всего мы бы даже не уловили качественную связь в наших данных."
      ],
      "metadata": {
        "id": "j9WBAjUCTimf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подумайте внимательно о токенизации, параметрах обучения"
      ],
      "metadata": {
        "id": "Q5vbdQh4PqtR"
      }
    }
  ]
}